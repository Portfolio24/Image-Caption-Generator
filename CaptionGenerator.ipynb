{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gAUS5Ph9B2w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZySqXLO_Ve_",
        "outputId": "1887fa43-c9a0-4fcb-aded-036072d631f7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJhotafK_7dV"
      },
      "outputs": [],
      "source": [
        "IMAGE_DIR = \"/content/drive/MyDrive/datasetml/images/\"\n",
        "CAPTIONS_FILE = \"/content/drive/MyDrive/datasetml/captions.json\"\n",
        "FEATURES_DIR = \"/content/drive/MyDrive/datasetml/features_npy2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO_KHAkZ_-RK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(FEATURES_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "id": "TppY6-IhAfqC",
        "outputId": "b3193161-2e36-4a87-dddb-dbdf00221c50"
      },
      "outputs": [],
      "source": [
        "# load vgg16 model\n",
        "model = VGG16()\n",
        "\n",
        "# restructure the model\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\n",
        "# summarize\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4fe5b6d795244445b7ade01713613ca2",
            "ddffca8b6d67484d84d9c6aae11a66b9",
            "1c9e6b202cb741b59f77d583b411159a",
            "44ae9c0bc12c40009e002c1adc731ee5",
            "228550fecd4a4f1fbafdec2b8a1cba5c",
            "ff7c8e13c14d489794d0a431838c957e",
            "5b48206a7416492c87541a3d667cb405",
            "576fea6c119b469dade95d17a45dd36a",
            "2b3460d82ff84b3b972537946dbdd7fa",
            "cc52b01278324f2d870e9c6f816d5087",
            "681eb82fa7bf44208d519d3cd9055744"
          ]
        },
        "id": "Ec8_D8H5EQJO",
        "outputId": "7b024cbb-2ed0-4677-f2f0-237db816a770"
      },
      "outputs": [],
      "source": [
        "# extract features from images (YOUR VERSION)\n",
        "\n",
        "features = {}\n",
        "\n",
        "for img_name in tqdm(os.listdir(IMAGE_DIR)):\n",
        "\n",
        "    # full image path\n",
        "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "\n",
        "    # load the image\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    # reshape data for model\n",
        "    image = image.reshape(\n",
        "        (1, image.shape[0], image.shape[1], image.shape[2])\n",
        "    )\n",
        "\n",
        "    # preprocess image for VGG\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    # extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "\n",
        "    # get image ID\n",
        "    image_id = img_name.split('.')[0]\n",
        "\n",
        "    # store feature (SAME AS HIS)\n",
        "    features[image_id] = feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oASlBYW-KbVU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "PICKLE_PATH = \"/content/drive/MyDrive/datasetml/features.pkl\"\n",
        "\n",
        "with open(PICKLE_PATH, 'wb') as f:\n",
        "    pickle.dump(features, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYuHrFVPKyRB"
      },
      "outputs": [],
      "source": [
        "with open(PICKLE_PATH, 'rb') as f:\n",
        "    features = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwjybOhuLTHJ",
        "outputId": "6fb9d6db-6fce-4667-bae5-14684027d8f9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(CAPTIONS_FILE, \"r\") as f:\n",
        "    raw_json = json.load(f)\n",
        "\n",
        "captions = {}\n",
        "\n",
        "for img, caption in raw_json.items():\n",
        "    cleaned = caption.strip().lower()\n",
        "    cleaned = cleaned.replace('\"', '').replace(\" ,\", \",\").replace(\" .\", \".\")\n",
        "    captions[img] = [\"startseq \" + cleaned + \" endseq\"]\n",
        "\n",
        "print(\"Total captions:\", len(captions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qAxC41wQF--",
        "outputId": "a0c980ca-3a5d-4fd2-c47f-42126fd21914"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean(captions):\n",
        "    for img_id, caps in captions.items():\n",
        "        for i in range(len(caps)):\n",
        "            caption = caps[i]\n",
        "\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "\n",
        "            # remove digits & special characters\n",
        "            caption = re.sub('[^a-z]', ' ', caption)\n",
        "\n",
        "            # remove extra spaces\n",
        "            caption = re.sub('\\s+', ' ', caption).strip()\n",
        "\n",
        "            # save back\n",
        "            caps[i] = caption\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-B9b1W6QJ4b",
        "outputId": "4287960d-8c2b-4d03-a2c6-538f7c0ef7ea"
      },
      "outputs": [],
      "source": [
        "clean(captions)\n",
        "print(list(captions.values())[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQBVVoMyRcTN"
      },
      "outputs": [],
      "source": [
        "all_captions = []\n",
        "\n",
        "for img_id in captions:\n",
        "    for caption in captions[img_id]:\n",
        "        all_captions.append(caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pWzSBEIRnSi",
        "outputId": "061f7bce-2094-4c74-db15-728a3e3567b6"
      },
      "outputs": [],
      "source": [
        "all_captions[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvxFLr6RSQAh",
        "outputId": "a6d94533-4f3b-4b4b-ffc8-2c5d85f3e10b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YRyIcQMTOs2",
        "outputId": "db9fc2c5-2501-4b73-cac3-bc31c71cd0a7"
      },
      "outputs": [],
      "source": [
        "# get maximum length of the caption available\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "\n",
        "max_length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpuPTcebaZIZ",
        "outputId": "c163217b-c517-4946-f764-01a90c0f51a0"
      },
      "outputs": [],
      "source": [
        "# FIX captions keys: remove .jpg extension\n",
        "captions = {\n",
        "    k.split('.')[0]: v\n",
        "    for k, v in captions.items()\n",
        "}\n",
        "\n",
        "# verify fix\n",
        "print(\"Sample feature keys:\", list(features.keys())[:3])\n",
        "print(\"Sample caption keys:\", list(captions.keys())[:3])\n",
        "\n",
        "# check intersection size\n",
        "print(\"Common keys:\", len(set(features.keys()).intersection(set(captions.keys()))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULgW-sc3ZVMN",
        "outputId": "0a986895-cb63-4afd-d76e-7b8c4c41f0d7"
      },
      "outputs": [],
      "source": [
        "common_keys = list(set(features.keys()).intersection(set(captions.keys())))\n",
        "\n",
        "split = int(len(common_keys) * 0.90)\n",
        "train = common_keys[:split]\n",
        "test  = common_keys[split:]\n",
        "\n",
        "print(\"Train size:\", len(train))\n",
        "print(\"Test size:\", len(test))\n",
        "print(\"Sample train ID:\", train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trhtvDunVGoh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "def data_generator(data_keys, captions, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    X1, X2, y = [], [], []\n",
        "    n = 0\n",
        "\n",
        "    while True:\n",
        "        for key in data_keys:\n",
        "            caps = captions[key]\n",
        "\n",
        "            for caption in caps:\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "                for i in range(1, len(seq)):\n",
        "                    in_seq = seq[:i]\n",
        "                    out_seq = seq[i]\n",
        "\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    out_seq = to_categorical(out_seq, num_classes=vocab_size)\n",
        "\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "                    n += 1\n",
        "\n",
        "                    if n == batch_size:\n",
        "                        # ðŸ”¥ IMPORTANT: yield TUPLE, not list\n",
        "                        yield (np.array(X1), np.array(X2)), np.array(y)\n",
        "                        X1, X2, y = [], [], []\n",
        "                        n = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "iqfCl8kwWCiU",
        "outputId": "b0c67190-1784-4639-8449-bc4f8f2e1852"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "\n",
        "# ðŸ”¥ FIX HERE\n",
        "se3 = LSTM(256, use_cudnn=False)(se2)\n",
        "\n",
        "# decoder\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam'\n",
        ")\n",
        "plot_model(model, show_shapes=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CeJ2ACpYJHn",
        "outputId": "0ddaf972-60b4-4f10-bbfb-fda2cc1c6789"
      },
      "outputs": [],
      "source": [
        "# training parameters\n",
        "epochs = 65\n",
        "batch_size = 64\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "# create generator\n",
        "generator = data_generator(\n",
        "    train,\n",
        "    captions,\n",
        "    features,\n",
        "    tokenizer,\n",
        "    max_length,\n",
        "    vocab_size,\n",
        "    batch_size\n",
        ")\n",
        "\n",
        "# train model (Keras 3 compatible)\n",
        "model.fit(\n",
        "    generator,\n",
        "    steps_per_epoch=steps,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax_FgIGVdv1I",
        "outputId": "1c8712b0-ce15-434f-e2c8-b28795f99864"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/datasetml\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "# save tokenizer\n",
        "with open(f\"{BASE_PATH}/tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# save max_length\n",
        "with open(f\"{BASE_PATH}/max_length.pkl\", \"wb\") as f:\n",
        "    pickle.dump(max_length, f)\n",
        "\n",
        "# save vocab_size\n",
        "with open(f\"{BASE_PATH}/vocab_size.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab_size, f)\n",
        "\n",
        "print(\"âœ… tokenizer, max_length, vocab_size saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld6VTJ0-fv7I",
        "outputId": "983c96f3-aad3-4b8a-fc0b-c22e74fb32a9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/datasetml/checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_PATH = f\"{CHECKPOINT_DIR}/caption_model_best.keras\"\n",
        "model.save(MODEL_PATH)\n",
        "\n",
        "print(\"âœ… Model saved at:\", MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UgB6K0AZvHO",
        "outputId": "991ab5b2-8272-4490-e814-e3288cc470c3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# build inference-safe model (non-cuDNN)\n",
        "def build_inference_model(vocab_size, max_length):\n",
        "    # image feature branch\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.4)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # caption branch\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.4)(se1)\n",
        "\n",
        "    # ðŸ”¥ IMPORTANT: disable cuDNN\n",
        "    se3 = LSTM(256, use_cudnn=False)(se2)\n",
        "\n",
        "    # decoder\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# build model\n",
        "model = build_inference_model(vocab_size, max_length)\n",
        "\n",
        "# load trained weights (NO retraining)\n",
        "model.load_weights(\n",
        "    \"/content/drive/MyDrive/datasetml/checkpoints/caption_model_best.keras\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Inference model loaded (non-cuDNN, weights reused)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r53FFH5MZvEp"
      },
      "outputs": [],
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kysn7Oo4sJfD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sample_top_k(preds, k=5):\n",
        "    preds = preds.flatten()\n",
        "    top_k_indices = np.argsort(preds)[-k:]\n",
        "    top_k_probs = preds[top_k_indices]\n",
        "    top_k_probs = top_k_probs / np.sum(top_k_probs)\n",
        "    return np.random.choice(top_k_indices, p=top_k_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw9c2FMbZvCB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    in_text = \"startseq\"\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        yhat = sample_top_k(yhat, k=5)   # ðŸ‘ˆ ONLY CHANGE\n",
        "\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "\n",
        "        if word is None:\n",
        "            break\n",
        "\n",
        "        in_text += \" \" + word\n",
        "\n",
        "        if word == \"endseq\":\n",
        "            break\n",
        "\n",
        "    return in_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsGgeD-lZuqv",
        "outputId": "d0a680db-03a3-45b9-cb34-f82eee49ddb9"
      },
      "outputs": [],
      "source": [
        " from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "# validate with test data\n",
        "actual, predicted = [], []\n",
        "\n",
        "for key in tqdm(test):\n",
        "    # get actual captions\n",
        "    caps = captions[key]\n",
        "\n",
        "    # predict caption for image\n",
        "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
        "\n",
        "    # split into words\n",
        "    y_pred = y_pred.split()\n",
        "\n",
        "    # remove start/end tokens from prediction\n",
        "    y_pred = [word for word in y_pred if word not in ['startseq', 'endseq']]\n",
        "\n",
        "    # prepare actual captions\n",
        "    actual_captions = []\n",
        "    for cap in caps:\n",
        "        cap_words = cap.split()\n",
        "        cap_words = [word for word in cap_words if word not in ['startseq', 'endseq']]\n",
        "        actual_captions.append(cap_words)\n",
        "\n",
        "    # append to lists\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)\n",
        "\n",
        "# calculate BLEU scores\n",
        "print(\"BLEU-1:\", corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "print(\"BLEU-2:\", corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UH8tnHu0uX1",
        "outputId": "6ab54572-422a-41a9-dc53-a2d011062c7c"
      },
      "outputs": [],
      "source": [
        "print(\"BLEU-3:\", corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))\n",
        "print(\"BLEU-4:\", corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB8uJBKzcc29"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def show_caption(image_name):\n",
        "    # remove extension to get image_id\n",
        "    image_id = image_name.split('.')[0]\n",
        "\n",
        "    # image path (your Drive structure)\n",
        "    img_path = os.path.join(\n",
        "        \"/content/drive/MyDrive/datasetml/images\",\n",
        "        image_name\n",
        "    )\n",
        "\n",
        "    # load image\n",
        "    image = Image.open(img_path)\n",
        "\n",
        "    # show actual captions\n",
        "    print(\"----------- Actual Captions -----------\")\n",
        "    for cap in captions[image_id]:\n",
        "        clean_cap = cap.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n",
        "        print(clean_cap)\n",
        "\n",
        "    # predict caption\n",
        "    y_pred = predict_caption(\n",
        "        model,\n",
        "        features[image_id],\n",
        "        tokenizer,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    y_pred = y_pred.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n",
        "\n",
        "    print(\"\\n----------- Predicted Caption -----------\")\n",
        "    print(y_pred)\n",
        "\n",
        "    # display image\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "G3IexEzTmE8c",
        "outputId": "13ce342d-56f4-44e7-c81e-b9598aa26907"
      },
      "outputs": [],
      "source": [
        "show_caption(\"1001545525.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "D-xtfdJ3nzrK",
        "outputId": "9e68913d-a7c4-4f0d-8f58-113d1f5475d6"
      },
      "outputs": [],
      "source": [
        "show_caption(\"1363843090.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "75wr9_BHoIMW",
        "outputId": "db17059d-27c2-4735-c4e3-63e4c430f559"
      },
      "outputs": [],
      "source": [
        "show_caption(\"1522787272.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "Q0f9TjtasozA",
        "outputId": "9fd43ac6-7aa8-4cc1-bca4-ffbc61c3bcd9"
      },
      "outputs": [],
      "source": [
        "show_caption(\"1561665101.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "3Si0jyp6s70O",
        "outputId": "fb8de615-0aaf-4ff5-cc6d-6c0a953bf47e"
      },
      "outputs": [],
      "source": [
        "show_caption(\"134206.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "LEVaW6Hat36c",
        "outputId": "0b5e72ec-2c9b-49cf-8ee1-98b0016977ab"
      },
      "outputs": [],
      "source": [
        "show_caption(\"1418019748.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "dSX4Fr83vIDC",
        "outputId": "1e5d3ba9-96ca-4d83-cda9-3be26dab40ef"
      },
      "outputs": [],
      "source": [
        "show_caption(\"1414779054.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNb4lrMewepj"
      },
      "source": [
        "new image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGXOj5Ezwgs7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# load VGG16 and remove classification layer\n",
        "vgg = VGG16()\n",
        "vgg = Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT0uhQM5wnMI"
      },
      "outputs": [],
      "source": [
        "def extract_features_new_image(image_path):\n",
        "    image = load_img(image_path, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    feature = vgg.predict(image, verbose=0)\n",
        "    return feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5RCyz6UwqS0"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def predict_new_image(image_path):\n",
        "    # extract image features\n",
        "    image_feature = extract_features_new_image(image_path)\n",
        "\n",
        "    # generate caption\n",
        "    caption = predict_caption(\n",
        "        model,\n",
        "        image_feature,\n",
        "        tokenizer,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    # clean caption\n",
        "    caption = caption.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n",
        "\n",
        "    # show image\n",
        "    image = Image.open(image_path)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Predicted Caption:\")\n",
        "    print(caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "qMGHPeD0wsvw",
        "outputId": "df1fad9b-ea69-494f-a48f-2c3ee98f1d8b"
      },
      "outputs": [],
      "source": [
        "# give ANY image path (new image)\n",
        "image_path = \"/content/dog.34.jpg\"\n",
        "# OR upload a new image and give its path\n",
        "\n",
        "predict_new_image(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "jXukcz3Qy6hz",
        "outputId": "494f7fc1-0118-40b5-824d-710b8dd1b4c6"
      },
      "outputs": [],
      "source": [
        "# give ANY image path (new image)\n",
        "image_path = \"/content/dog.39.jpg\"\n",
        "# OR upload a new image and give its path\n",
        "\n",
        "predict_new_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Koy6D2Iq6JdA",
        "outputId": "24f518a0-0746-4a68-ea7b-b8838a33a492"
      },
      "outputs": [],
      "source": [
        "# give ANY image path (new image)\n",
        "image_path = \"/content/test1.jpg\"\n",
        "# OR upload a new image and give its path\n",
        "\n",
        "predict_new_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "IeuLauZT691E",
        "outputId": "f6e4ba5e-f5ca-44a1-98e1-9017a0269825"
      },
      "outputs": [],
      "source": [
        "# give ANY image path (new image)\n",
        "image_path = \"/content/test2.jpg\"\n",
        "# OR upload a new image and give its path\n",
        "\n",
        "predict_new_image(image_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c9e6b202cb741b59f77d583b411159a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_576fea6c119b469dade95d17a45dd36a",
            "max": 15000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b3460d82ff84b3b972537946dbdd7fa",
            "value": 15000
          }
        },
        "228550fecd4a4f1fbafdec2b8a1cba5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3460d82ff84b3b972537946dbdd7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44ae9c0bc12c40009e002c1adc731ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc52b01278324f2d870e9c6f816d5087",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_681eb82fa7bf44208d519d3cd9055744",
            "value": "â€‡15000/15000â€‡[29:55&lt;00:00,â€‡11.02it/s]"
          }
        },
        "4fe5b6d795244445b7ade01713613ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddffca8b6d67484d84d9c6aae11a66b9",
              "IPY_MODEL_1c9e6b202cb741b59f77d583b411159a",
              "IPY_MODEL_44ae9c0bc12c40009e002c1adc731ee5"
            ],
            "layout": "IPY_MODEL_228550fecd4a4f1fbafdec2b8a1cba5c"
          }
        },
        "576fea6c119b469dade95d17a45dd36a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b48206a7416492c87541a3d667cb405": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "681eb82fa7bf44208d519d3cd9055744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc52b01278324f2d870e9c6f816d5087": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddffca8b6d67484d84d9c6aae11a66b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff7c8e13c14d489794d0a431838c957e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5b48206a7416492c87541a3d667cb405",
            "value": "100%"
          }
        },
        "ff7c8e13c14d489794d0a431838c957e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
